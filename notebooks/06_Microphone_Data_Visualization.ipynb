{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microphone Data Visualization for Report\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook visualizes and explains the microphone data used in the acoustic navigation experiment.\n",
    "\n",
    "**Key Points:**\n",
    "- Each microphone records **RAW PRESSURE TIME-SERIES** (NOT FFT or spectrograms)\n",
    "- 8 microphones arranged in a circular array around the agent\n",
    "- Time-domain acoustic data captures both direct sound and reflections\n",
    "- Spatial-temporal patterns encode direction to goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from src.lmdb_dataset import LMDBAcousticDataset\n",
    "from src.cave_dataset import ACTION_NAMES, MIC_OFFSETS\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LMDBAcousticDataset('D:/audiomaze_lmdb_100')\n",
    "print(f\"Total samples: {len(dataset):,}\")\n",
    "print(f\"Mic data shape per sample: (8, 11434)\")\n",
    "print(f\"\\nAction distribution: {dataset.metadata['action_counts']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Format Explanation\n",
    "\n",
    "### What is the microphone data?\n",
    "\n",
    "**Data Type:** Raw acoustic pressure time-series in the **time domain**\n",
    "\n",
    "**Shape:** `(8, 11434)` where:\n",
    "- **8 channels** = 8 microphones in circular array\n",
    "- **11,434 samples** = time-series samples per microphone\n",
    "\n",
    "**Processing:**\n",
    "- Per-sample normalization: mean = 0, std = 1\n",
    "- Removes absolute amplitude, preserves relative differences\n",
    "\n",
    "**Microphone Array Layout:**\n",
    "```\n",
    "Mic positions (relative to agent):\n",
    "  7 (Up-Right)     6 (Up)        5 (Up-Left)\n",
    "  \n",
    "  0 (Right)      AGENT          4 (Left)\n",
    "  \n",
    "  1 (Down-Right)   2 (Down)      3 (Down-Left)\n",
    "```\n",
    "\n",
    "**Acoustic Simulation:**\n",
    "- Simulator: k-Wave (MATLAB acoustic toolbox)\n",
    "- Signal: 150 kHz ultrasonic tone burst (6 cycles)\n",
    "- Source: Goal position (sound emanates from target)\n",
    "- Recording: Full pressure field over time at all grid points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one sample to inspect\n",
    "sample_idx = 1000\n",
    "mic_data, action, file_idx, position = dataset[sample_idx]\n",
    "action_name = ACTION_NAMES[int(action.item())]\n",
    "\n",
    "print(f\"Sample {sample_idx}:\")\n",
    "print(f\"  Mic data shape: {mic_data.shape}\")\n",
    "print(f\"  Data type: {mic_data.dtype}\")\n",
    "print(f\"  Action label: {action_name}\")\n",
    "print(f\"  Position: ({int(position[0])}, {int(position[1])})\")\n",
    "print(f\"  File index: {int(file_idx)}\")\n",
    "print(f\"\\nData statistics (after normalization):\")\n",
    "print(f\"  Mean: {mic_data.mean():.6f} (should be ~0)\")\n",
    "print(f\"  Std: {mic_data.std():.6f} (should be ~1)\")\n",
    "print(f\"  Min: {mic_data.min():.3f}\")\n",
    "print(f\"  Max: {mic_data.max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Single Microphone Visualization ⭐\n",
    "\n",
    "This is the **primary figure** for understanding what each microphone records.\n",
    "\n",
    "The plot shows:\n",
    "- **Raw pressure time-series** from one microphone (Mic 0: Right)\n",
    "- **Early reflections** (0-50ms): Direct sound + first wall bounces → encodes direction\n",
    "- **Late reverberation** (50-145ms): Complex reflections → encodes cave geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time axis\n",
    "dt = 1.27e-5  # seconds per sample (from k-Wave simulation)\n",
    "time_axis = np.arange(11434) * dt\n",
    "\n",
    "# Plot single microphone\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(time_axis * 1000, mic_data[0].numpy(), linewidth=0.5, color='steelblue')\n",
    "plt.xlabel('Time (milliseconds)', fontsize=13)\n",
    "plt.ylabel('Normalized Pressure', fontsize=13)\n",
    "plt.title('Microphone 0 (Right): Raw Acoustic Pressure Time-Series', fontsize=15, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotate regions\n",
    "plt.axvspan(0, 50, alpha=0.15, color='green', label='Early reflections (directional cues)')\n",
    "plt.axvspan(50, 145, alpha=0.15, color='orange', label='Late reverberation (cave geometry)')\n",
    "plt.legend(loc='upper right', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Recording duration: {time_axis[-1]*1000:.1f} milliseconds\")\n",
    "print(f\"Sampling rate: {1/dt/1000:.1f} kHz\")\n",
    "print(f\"Total samples: {len(time_axis):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. All 8 Microphones Comparison\n",
    "\n",
    "This shows that different microphones receive **different acoustic signatures** based on:\n",
    "- Distance to sound source (goal)\n",
    "- Wall reflections\n",
    "- Directional arrival patterns\n",
    "\n",
    "The differences between microphones provide spatial information that the neural network learns to decode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(8, 1, figsize=(14, 12), sharex=True)\n",
    "mic_labels = ['Right', 'Down-Right', 'Down', 'Down-Left', 'Left', 'Up-Left', 'Up', 'Up-Right']\n",
    "\n",
    "for i in range(8):\n",
    "    axes[i].plot(time_axis * 1000, mic_data[i].numpy(), linewidth=0.5, color=f'C{i}')\n",
    "    axes[i].set_ylabel(f'Mic {i}\\n{mic_labels[i]}', fontsize=10, rotation=0, ha='right', va='center')\n",
    "    axes[i].grid(True, alpha=0.2)\n",
    "    axes[i].set_ylim(-4, 4)  # Consistent scale for comparison\n",
    "\n",
    "axes[-1].set_xlabel('Time (milliseconds)', fontsize=12)\n",
    "fig.suptitle('All 8 Microphones: Spatial Diversity in Time-Series', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Action label for this sample: {action_name}\")\n",
    "print(f\"\\nObservation: Each microphone shows a unique waveform pattern.\")\n",
    "print(f\"This spatial diversity encodes the direction to the goal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Zoomed View: Direct Sound and Early Reflections\n",
    "\n",
    "The first ~25 milliseconds contain the most important **directional information**:\n",
    "- Direct sound arrives first at mics closest to goal\n",
    "- Early reflections have less interference\n",
    "- Amplitude differences are most pronounced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on first 2000 samples (~25ms)\n",
    "zoom_samples = 2000\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "for i in range(8):\n",
    "    ax.plot(time_axis[:zoom_samples] * 1000, mic_data[i, :zoom_samples].numpy(),\n",
    "            label=mic_labels[i], linewidth=1.2, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Time (milliseconds)', fontsize=12)\n",
    "ax.set_ylabel('Normalized Pressure', fontsize=12)\n",
    "ax.set_title('Zoomed View: Direct Sound and Early Reflections (First 25ms)', fontsize=15, fontweight='bold')\n",
    "ax.legend(ncol=4, fontsize=10, loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Time window: 0 to {time_axis[zoom_samples-1]*1000:.1f} ms\")\n",
    "print(f\"This region contains the strongest directional cues for navigation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Directional Encoding: How the Array Encodes Goal Direction\n\nThis demonstration shows that the 8-microphone array pattern **changes based on action label**.\n\nWe compare samples with different actions (UP/DOWN/LEFT/RIGHT) and visualize:\n- **RMS energy of early reflections (first 50ms)** at each microphone\n- **Polar plots** showing spatial pattern\n- How the \"signature\" changes with goal direction\n\n**Note**: We use early-time RMS energy instead of max amplitude because:\n- Max amplitude can occur anywhere in the 145ms recording (often from late reflections)\n- Early reflections (0-50ms) contain the strongest directional information\n- RMS energy better captures the overall signal strength in the directional window"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find samples with different actions\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Collect samples by action (skip STOP=0, focus on directional actions)\n",
    "samples_by_action = {1: [], 2: [], 3: [], 4: []}  # UP, DOWN, LEFT, RIGHT\n",
    "for idx in range(len(dataset)):\n",
    "    _, action, _, _ = dataset[idx]\n",
    "    action_int = int(action.item())\n",
    "    if action_int in samples_by_action and len(samples_by_action[action_int]) < 5:\n",
    "        samples_by_action[action_int].append(idx)\n",
    "    if all(len(v) >= 5 for v in samples_by_action.values()):\n",
    "        break\n",
    "\n",
    "print(f\"Found samples for each action:\")\n",
    "for action_id, indices in samples_by_action.items():\n",
    "    print(f\"  {ACTION_NAMES[action_id]}: {len(indices)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot 8-mic amplitude patterns for each direction\nfig = plt.figure(figsize=(14, 10))\naction_names_4class = ['UP', 'DOWN', 'LEFT', 'RIGHT']\nangles = np.linspace(0, 2*np.pi, 8, endpoint=False)\n\n# Use early-time window (first 50ms = ~3900 samples)\ndt = 1.27e-5\nearly_time_samples = int(0.05 / dt)  # 50ms\n\nfor i, (action_id, sample_indices) in enumerate(samples_by_action.items()):\n    idx = sample_indices[0]  # Use first sample\n    mic_data_sample, _, _, _ = dataset[idx]\n\n    # Compute RMS energy of early reflections (first 50ms) for each mic\n    early_rms = []\n    for j in range(8):\n        early_signal = mic_data_sample[j, :early_time_samples].numpy()\n        rms = float(np.sqrt(np.mean(early_signal**2)))\n        early_rms.append(rms)\n\n    # Polar plot\n    ax = plt.subplot(2, 2, i+1, projection='polar')\n    ax.plot(angles, early_rms, 'o-', linewidth=2.5, markersize=10, color=f'C{i}')\n    ax.fill(angles, early_rms, alpha=0.2, color=f'C{i}')\n    \n    # Set 0° = Right (East)\n    ax.set_theta_zero_location('E')\n    ax.set_theta_direction(1)  # Counterclockwise\n    \n    # Label microphones\n    ax.set_xticks(angles)\n    ax.set_xticklabels(mic_labels, fontsize=9)\n    \n    ax.set_title(f'Action: {action_names_4class[i]}', fontsize=14, fontweight='bold', pad=20)\n    ax.grid(True, alpha=0.3)\n\nplt.suptitle('Directional Encoding: Early-Time RMS Energy per Microphone (0-50ms)', fontsize=16, fontweight='bold', y=0.98)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nInterpretation:\")\nprint(f\"- Each action creates a spatial pattern in the early-time window\")\nprint(f\"- The neural network learns to extract these directional features\")\nprint(f\"- Early reflections (0-50ms) provide the strongest directional cues\")\nprint(f\"\\nNote: In caves with complex reflections, directional encoding may be subtle\")\nprint(f\"The CNN learns to extract these patterns automatically from raw waveforms\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Acoustic Simulation Parameters\n",
    "\n",
    "### Technical Details\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| **Signal Type** | 6-cycle tone burst | Short ultrasonic pulse |\n",
    "| **Frequency** | 150 kHz | Ultrasonic range (inaudible to humans) |\n",
    "| **Duration** | 0.145 seconds | Total recording time |\n",
    "| **Samples** | 11,434 | Time samples per microphone |\n",
    "| **Sampling interval (dt)** | 1.27e-5 s | ~78.7 kHz effective sampling rate |\n",
    "| **Sound speed (air)** | 343 m/s | Standard atmospheric conditions |\n",
    "| **Sound speed (walls)** | 220 m/s | Slower propagation → more reflections |\n",
    "| **Grid resolution** | 0.01 m | 1 cm per grid cell |\n",
    "| **Simulator** | k-Wave | MATLAB acoustic wave propagation toolbox |\n",
    "| **Physics** | 2D wave equation | With absorption and reflections |\n",
    "\n",
    "### Why 150 kHz Ultrasonic?\n",
    "- **Higher frequency** = shorter wavelength (λ = 2.3 mm at 150 kHz)\n",
    "- Better spatial resolution for navigation\n",
    "- Similar to bat echolocation (20-120 kHz)\n",
    "\n",
    "### Why Time-Domain (not FFT)?\n",
    "- **Time-domain preserves phase information** (critical for direction)\n",
    "- FFT would lose temporal arrival differences between mics\n",
    "- End-to-end learning from raw waveforms is more flexible\n",
    "- Model learns optimal features automatically (like in speech recognition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Figures for Report\n",
    "\n",
    "Save high-quality figures for inclusion in technical reports/papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path('../figures')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Re-create single microphone figure for export\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(time_axis * 1000, mic_data[0].numpy(), linewidth=0.5, color='steelblue')\n",
    "plt.xlabel('Time (milliseconds)', fontsize=13)\n",
    "plt.ylabel('Normalized Pressure', fontsize=13)\n",
    "plt.title('Raw Acoustic Pressure Time-Series (Single Microphone)', fontsize=15, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvspan(0, 50, alpha=0.15, color='green', label='Early reflections')\n",
    "plt.axvspan(50, 145, alpha=0.15, color='orange', label='Late reverberation')\n",
    "plt.legend(loc='upper right', fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'mic_single_timeseries.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(output_dir / 'mic_single_timeseries.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved: {output_dir / 'mic_single_timeseries.png'} (300 DPI)\")\n",
    "print(f\"✓ Saved: {output_dir / 'mic_single_timeseries.pdf'} (vector)\")\n",
    "print(f\"\\nThese figures are ready for inclusion in your report!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings for Report\n",
    "\n",
    "1. **Data Format**: Raw acoustic pressure time-series (NOT FFT/frequency-domain)\n",
    "   - Shape: (8 microphones, 11,434 time samples)\n",
    "   - Duration: 145 milliseconds\n",
    "   - Domain: Time-domain waveforms\n",
    "\n",
    "2. **Microphone Array**: 8 mics in circular arrangement (1 grid cell radius)\n",
    "   - Each mic independently records pressure wave\n",
    "   - Spatial diversity: correlation ~0.01-0.03 between adjacent mics\n",
    "\n",
    "3. **Spatial Encoding**: Different mics receive different amplitudes/phases\n",
    "   - Early reflections (0-50ms) contain strongest directional cues\n",
    "   - Max amplitude patterns vary with goal direction\n",
    "   - Polar plots show distinct signatures for UP/DOWN/LEFT/RIGHT\n",
    "\n",
    "4. **Temporal Encoding**: Time-series captures rich information\n",
    "   - Direct sound arrival + early reflections → direction\n",
    "   - Late reverberation → cave geometry and obstacles\n",
    "\n",
    "5. **Physics-Based Simulation**: k-Wave acoustic simulator\n",
    "   - 150 kHz ultrasonic signal for high spatial resolution\n",
    "   - Realistic wall reflections and absorption\n",
    "   - 2D wave propagation in complex cave environments\n",
    "\n",
    "### Implications for Machine Learning\n",
    "- CNN learns to extract spatial-temporal features from raw waveforms\n",
    "- No manual feature engineering (FFT, spectrograms, etc.)\n",
    "- End-to-end learning similar to modern speech recognition\n",
    "- Model discovers optimal features for navigation task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}