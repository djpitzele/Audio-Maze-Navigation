"""
LMDB-backed dataset for fast acoustic navigation training.

This dataset loads pre-converted LMDB data generated by convert_to_lmdb.py.
Provides 10-100x faster initialization and data loading compared to HDF5.
"""

import lmdb
import pickle
import json
import numpy as np
import torch
from pathlib import Path
from torch.utils.data import Dataset
from typing import Dict


class LMDBAcousticDataset(Dataset):
    """
    Fast LMDB-backed dataset for acoustic navigation.

    Features:
    - Instant initialization (<1 second)
    - Memory-mapped file access (OS handles caching)
    - Multi-worker safe
    - Scales to large datasets (1000+ mazes, 150GB+)

    Args:
        lmdb_dir: Directory containing data.lmdb and metadata.json
        normalize: Apply per-sample normalization (default: True)
    """

    def __init__(self, lmdb_dir: str, normalize: bool = True):
        self.lmdb_dir = Path(lmdb_dir)
        self.normalize = normalize

        # Load metadata
        metadata_path = self.lmdb_dir / "metadata.json"
        if not metadata_path.exists():
            raise FileNotFoundError(f"Metadata not found: {metadata_path}")

        with open(metadata_path, 'r') as f:
            self.metadata = json.load(f)

        self.num_samples = self.metadata['num_samples']
        self.action_map = self.metadata['action_map']
        self.action_names = ['STOP', 'UP', 'DOWN', 'LEFT', 'RIGHT']

        # Open LMDB environment (read-only, memory-mapped)
        lmdb_path = self.lmdb_dir / "data.lmdb"
        if not lmdb_path.exists():
            raise FileNotFoundError(f"LMDB database not found: {lmdb_path}")

        self.env = lmdb.open(
            str(lmdb_path),
            readonly=True,
            lock=False,
            readahead=False,  # Disable for random access
            meminit=False,
        )

        print(f"Loaded LMDB dataset: {self.num_samples:,} samples")
        print(f"Action distribution: {self.metadata['action_counts']}")

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        # Read from LMDB
        with self.env.begin(write=False) as txn:
            value = txn.get(str(idx).encode('ascii'))
            if value is None:
                raise IndexError(f"Sample {idx} not found in LMDB")
            sample = pickle.loads(value)

        # Extract data
        mic_data = sample['mic_data']  # (8, 11434) float32
        action = sample['action']
        file_idx = sample['file_idx']
        position = sample['position']

        # Per-sample normalization (critical for generalization)
        if self.normalize:
            mic_mean = mic_data.mean()
            mic_std = mic_data.std()
            if mic_std > 1e-8:
                mic_data = (mic_data - mic_mean) / mic_std
            else:
                mic_data = mic_data - mic_mean

        # Convert to tensors
        return (
            torch.from_numpy(mic_data),
            torch.tensor(action, dtype=torch.long),
            torch.tensor(file_idx, dtype=torch.long),
            torch.tensor(position, dtype=torch.long),
        )

    def get_sample_with_position(self, idx):
        """Get sample with human-readable position info."""
        mic, action, file_idx, pos = self.__getitem__(idx)
        return mic, action, (int(pos[0]), int(pos[1])), int(file_idx)

    def close(self):
        """Close LMDB environment."""
        if hasattr(self, 'env'):
            self.env.close()

    def __del__(self):
        """Cleanup on deletion."""
        self.close()


def compute_class_distribution_lmdb(dataset) -> Dict[str, int]:
    """Compute class distribution from LMDB dataset metadata."""
    return dataset.metadata['action_counts']


def compute_class_weights_lmdb(counts: Dict[str, int]) -> torch.Tensor:
    """Compute inverse frequency class weights."""
    action_names = ['stop', 'up', 'down', 'left', 'right']
    arr = np.array([counts.get(name, 1) for name in action_names], dtype=np.float32)
    total = arr.sum()
    weights = total / (len(action_names) * arr)
    return torch.tensor(weights, dtype=torch.float32)


# For backward compatibility with HDF5 dataset
ACTION_MAP = {'stop': 0, 'up': 1, 'down': 2, 'left': 3, 'right': 4, '': -1}
ACTION_NAMES = ['STOP', 'UP', 'DOWN', 'LEFT', 'RIGHT']
MIC_OFFSETS = [
    (0, 1), (1, 1), (1, 0), (1, -1),
    (0, -1), (-1, -1), (-1, 0), (-1, 1)
]
